{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from scipy.stats import shapiro, kruskal\n",
    "import scikit_posthocs as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Files from Folder: Distance Analysis\n",
    "This script combines distance matrices from multiple Excel files in a specified folder.\n",
    "It reads the first two rows of each file, drops the first column, transposes the data,\n",
    "and appends the 'Well' information extracted from the filename to each row.\n",
    "The combined data is then saved to a new Excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script combines distance matrices from multiple Excel files in a specified folder.\n",
    "Each file should contain a distance matrix with the first two rows representing segments and their values.\"\"\"\n",
    "\n",
    "def combine_distance_matrices(folder_path):\n",
    "    \"\"\"\n",
    "    Combine distance matrices from multiple Excel files in a specified folder.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    # Iterate through all files in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Only process Excel files that contain 'DistanceMatrix' in their name\n",
    "        if filename.endswith(\".xlsx\") and \"DistanceMatrix\" in filename:\n",
    "            try:\n",
    "                # Extract the 'Well' identifier from the filename (assumes it's the second part)\n",
    "                well = filename.split('_')[1]\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                # Read only the first two rows (since that's where the data is)\n",
    "                df = pd.read_excel(file_path, engine='openpyxl', header=None, nrows=2)\n",
    "\n",
    "                # Drop the first column (usually an index or label column)\n",
    "                df = df.iloc[:, 1:]\n",
    "\n",
    "                # Transpose the DataFrame so that each segment/value pair is a row\n",
    "                transposed = df.transpose()\n",
    "                transposed.columns = ['Segment', 'Value']\n",
    "                transposed['Well'] = well\n",
    "\n",
    "                # Add the processed DataFrame to our list\n",
    "                combined_data.append(transposed)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Print an error message if something goes wrong with this file\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "    # Concatenate all the individual DataFrames into one\n",
    "    if combined_data:\n",
    "        result_df = pd.concat(combined_data, ignore_index=True)\n",
    "        result_df = result_df[['Well', 'Segment', 'Value']]\n",
    "        return result_df\n",
    "    else:\n",
    "        # If no valid files were found, return an empty DataFrame with the correct columns\n",
    "        print(\"No valid files found.\")\n",
    "        return pd.DataFrame(columns=['Well', 'Segment', 'Value'])\n",
    "\n",
    "# Main script: set folder and output paths, run the combine function, and save the result\n",
    "\n",
    "folder_path = \"TYPE IN FOLDER PATH\"  # Update this to your actual path\n",
    "output_path = os.path.join(folder_path, \"combined_distance_matrix.xlsx\")\n",
    "print(f\"Combining distance matrices from: {folder_path}\")\n",
    "print(f\"Output will be saved to: {output_path}\")\n",
    "combined_df = combine_distance_matrices(folder_path)\n",
    "combined_df.to_excel(output_path, index=False)\n",
    "print(f\"Combined file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of LY distance results \n",
    "Only LY+ cells located within 150 um from the edge of the wound will be counted, then classified based on the treatment condition.\n",
    "The filtered results are then stored in another Excel file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Command to sort out results from 0-150 um, grouped by treatment\n",
    "\n",
    "\"\"\" This section filters the combined DataFrame to keep only values between 0 and 150, and assigns group labels based on 'Well' \"\"\"\n",
    "\n",
    "# Filter the combined DataFrame to keep only values between 0 and 150, and assign group labels based on 'Well'\n",
    "# Use the already loaded combined_df\n",
    "df = combined_df.copy()\n",
    "\n",
    "# Convert Value to numeric (if not already)\n",
    "df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
    "\n",
    "# Remove all negative values\n",
    "df = df[df['Value'] >= 0]\n",
    "# Remove all values > 150\n",
    "df = df[df['Value'] <= 150]\n",
    "\n",
    "# Convert Well to integer if possible\n",
    "df['Well'] = pd.to_numeric(df['Well'], errors='coerce')\n",
    "\n",
    "# Remove rows where Well could not be converted\n",
    "df = df.dropna(subset=['Well'])\n",
    "\n",
    "# Make sure 'Well' is treated as integer\n",
    "df['Well'] = df['Well'].astype(int)\n",
    "\n",
    "# Assign a group number: every 3 wells (because 3 wells per repeat x 2 repeats) will be in the same group\n",
    "df['Well_Group'] = (df['Well'] // 3) * 7  # Groups: 0, 7, 14, etc.\n",
    "\n",
    "# Define your custom labels for each group\n",
    "group_label_map = {\n",
    "    0: \"UT\",\n",
    "    7: \"DBD\",\n",
    "    14: \"H2O2\",\n",
    "    21: \"Doxo\",\n",
    "    28: \"SNAP\",\n",
    "    35: \"FA\",\n",
    "    42: \"DTT\",\n",
    "}\n",
    "# Map numeric group to your custom label\n",
    "df['Well_Group_Label'] = df['Well_Group'].map(group_label_map)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the results to Excel in the same folder as the input files\n",
    "output_path = os.path.join(folder_path, \"filtered_distance_matrix_results.xlsx\")\n",
    "df.to_excel(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Summary of the script:\n",
    "# - The script combines distance matrices from multiple Excel files in a specified folder.\n",
    "# - Data is filtered and grouped efficiently using pandas vectorized operations.\n",
    "# - Grouping logic is clear and uses integer division for mapping wells to treatments.\n",
    "# - Mapping group numbers to human-readable labels improves downstream analysis and plotting.\n",
    "# - Data is exported for reproducibility and further use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of Distance results: create a heatmap from filtered results obtained in the previous cell\n",
    "This script creates a heatmap from the filtered distance matrix results.\n",
    "It bins the 'Value' column into specified ranges (steps of 10) and counts occurrences for each 'Well_Group_Label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script combines distance matrices from multiple Excel files in a specified folder.\n",
    "Each file should contain a distance matrix with the first two rows representing segments and their values.\"\"\"\n",
    "\n",
    "# Define folder and file path\n",
    "folder_path = \"TYPE IN FOLDER PATH\"  # Update FOLDER PATH IF NEEDED\n",
    "results_file = os.path.join(folder_path, \"filtered_distance_matrix_results.xlsx\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(results_file, engine='openpyxl')\n",
    "\n",
    "# Convert Value to numeric (if not already)\n",
    "df['Value'] = pd.to_numeric(df['Value'], errors='coerce')\n",
    "\n",
    "# Bin the Value column (0-150, e.g., in steps of 10)\n",
    "bins = list(range(0, 121, 10))\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "df['Value_bin'] = pd.cut(df['Value'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Create a pivot table: rows=Well_Group_Label, columns=Value_bin, values=count\n",
    "heatmap_data = df.pivot_table(columns='Well_Group_Label', index='Value_bin', values='Value', aggfunc='count', fill_value=0)\n",
    "\n",
    "# Desired column order\n",
    "desired_order = [\"UT\", \"DBD\", \"H2O2\", \"Doxo\", \"SNAP\", \"FA\", \"DTT\"]\n",
    "\n",
    "# Reindex columns to desired order (missing columns will be filled with 0)\n",
    "heatmap_data = heatmap_data.reindex(columns=desired_order, fill_value=0)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"plasma\", annot=True, fmt=\"d\")\n",
    "plt.title(\"Number of LY+ cells from scratch (0-120 µm) by Treatment\")\n",
    "plt.xlabel(\"Treatments\")\n",
    "plt.ylabel(\"Distance from scratch (µm)\")\n",
    "plt.tight_layout()\n",
    "plt.gca().invert_yaxis()  # Invert the Y axis\n",
    "\n",
    "# Save figure (in the same folder as your data)\n",
    "figure_path = os.path.join(folder_path, \"distance_heatmap.png\")\n",
    "plt.savefig(figure_path, dpi=500)\n",
    "print(f\"Figure saved to: {figure_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine GJ Green area files and make barplot \n",
    "This script processes multiple Excel files in a specified folder, extracts GJ Green Area and Intersect Area data, and combines the results into a single DataFrame. The final DataFrame is saved to an Excel file.\n",
    "It also generates a barplot showing the Mean + SD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script processes multiple Excel files containing GJ Green Area and Intersect Area data, calculates the corrected green area, groups wells by treatment, and visualizes the results using a bar plot and strip plot.\n",
    "\"\"\"\n",
    "\n",
    "# Define folder containing the files\n",
    "folder_path = \"TYPE IN FOLDER PATH\" # <-- Update this to your folder path\n",
    "\n",
    "# Find all Excel files containing 'objectfeatures' in the filename\n",
    "file_paths = glob(os.path.join(folder_path, \"*objectfeatures*.xlsx\"))\n",
    "\n",
    "# Initialize list to collect individual result DataFrames\n",
    "all_results = []\n",
    "\n",
    "# Define your custom labels\n",
    "group_label_map = {\n",
    "    0: \"UT\",\n",
    "    7: \"DBD\",\n",
    "    14: \"H2O2\",\n",
    "    21: \"Doxo\",\n",
    "    28: \"SNAP\",\n",
    "    35: \"FA\",\n",
    "    42: \"DTT\",\n",
    "}\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df_green = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "        # Filter Green Area\n",
    "        green_area_df = df_green[df_green['Name'].str.contains('GJ Green area', na=False)]\n",
    "        green_area_df = green_area_df[~green_area_df['Name'].str.contains('Intersect', na=False)]\n",
    "        green_area_sum = green_area_df['Area, 2D Oriented Bounds (µm²)'].sum()\n",
    "\n",
    "        # Filter Intersect Area\n",
    "        intersect_area_df = df_green[df_green['Name'].str.contains('Intersect', na=False)]\n",
    "        intersect_area_sum = intersect_area_df['Area, 2D Oriented Bounds (µm²)'].sum()\n",
    "\n",
    "        # Subtract intersect area from green area\n",
    "        green_area_corrected = green_area_sum - intersect_area_sum\n",
    "\n",
    "        # Extract well name/number from file name\n",
    "        well_name = os.path.basename(file_path).split('_')[1]  # Assuming well name is the second part of the filename\n",
    "        try:\n",
    "            well_num = int(well_name)\n",
    "        except ValueError:\n",
    "            well_num = None\n",
    "\n",
    "        if well_num is not None:\n",
    "            well_group = (well_num // 3) * 7\n",
    "            well_group_label = group_label_map.get(well_group, str(well_group))\n",
    "        else:\n",
    "            well_group = None\n",
    "            well_group_label = None\n",
    "            \n",
    "        # Create result DataFrame\n",
    "        df_results = pd.DataFrame({\n",
    "            'Well': [well_name],\n",
    "            'Well_Group': [well_group],\n",
    "            'Well_Group_Label': [well_group_label],\n",
    "            'Sum Green Area': [green_area_sum],\n",
    "            'Sum Intersect Area': [intersect_area_sum],\n",
    "            'Sum Green Area (corrected)': [green_area_corrected]\n",
    "        })\n",
    "\n",
    "        all_results.append(df_results)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "combined_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Save to Excel\n",
    "output_file = os.path.join(folder_path, \"combined_green_area_results.xlsx\")\n",
    "combined_results.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Saved combined results to: {output_file}\")\n",
    "\n",
    "# Show all unique types in the column\n",
    "print(\"Unique types in 'Sum Green Area (corrected)':\", combined_results['Sum Green Area (corrected)'].apply(type).unique())\n",
    "\n",
    "# Show all non-numeric values (if any)\n",
    "non_numeric = combined_results[~combined_results['Sum Green Area (corrected)'].apply(lambda x: isinstance(x, (int, float)))]\n",
    "print(\"Non-numeric rows:\")\n",
    "print(non_numeric[['Well_Group_Label', 'Sum Green Area (corrected)']])\n",
    "\n",
    "#Plotting the results\n",
    "\n",
    "# Clean column names\n",
    "combined_results.columns = combined_results.columns.str.strip()\n",
    "\n",
    "# Remove rows with missing group labels or values\n",
    "combined_results = combined_results[combined_results['Well_Group_Label'].notna()]\n",
    "\n",
    "# Force conversion to numeric, drop all non-numeric\n",
    "combined_results['Sum Green Area (corrected)'] = pd.to_numeric(\n",
    "    combined_results['Sum Green Area (corrected)'], errors='coerce'\n",
    ")\n",
    "combined_results = combined_results.dropna(subset=['Sum Green Area (corrected)'])\n",
    "\n",
    "# Confirm dtype is now float\n",
    "print(\"Dtype after conversion:\", combined_results['Sum Green Area (corrected)'].dtype)\n",
    "\n",
    "# Specify the desired order of treatments\n",
    "order = [\"UT\", \"DBD\", \"H2O2\", \"Doxo\", \"SNAP\", \"FA\", \"DTT\"]\n",
    "\n",
    "# Barplot of Sum Green Area (corrected) by Well_Group_Label\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=combined_results,\n",
    "    x='Well_Group_Label',\n",
    "    y='Sum Green Area (corrected)',\n",
    "    ci='sd',\n",
    "    palette='viridis',\n",
    "    order=order  # Specify the order here\n",
    ")\n",
    "\n",
    "sns.stripplot(\n",
    "    data=combined_results,\n",
    "    x='Well_Group_Label',\n",
    "    y='Sum Green Area (corrected)',\n",
    "    color='black',\n",
    "    size=6,\n",
    "    order=order,\n",
    "    jitter=True,\n",
    "    dodge=True,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Corrected GJ Green Area by Treatment')\n",
    "plt.xlabel('Treatment')\n",
    "plt.ylabel('Sum Green Area (corrected) [µm²]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting LY+ area - statistical analysis\n",
    "This code makes a barplot of the combined results, removes outliers, test for normality of the data and runs the most appropiate statistical analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "file_path = \"TYPE IN FILE PATH\"\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove rows with missing group labels or values\n",
    "df = df[df['Well_Group_Label'].notna()]\n",
    "\n",
    "# Convert to numeric, drop non-numeric\n",
    "df['Sum Green Area (corrected)'] = pd.to_numeric(df['Sum Green Area (corrected)'], errors='coerce')\n",
    "df = df.dropna(subset=['Sum Green Area (corrected)'])\n",
    "\n",
    "# Specify the desired order of treatments (edit as needed)\n",
    "order = [\"UT\", \"DBD\", \"H2O2\", \"Doxo\", \"SNAP\", \"FA\", \"DTT\"]\n",
    "\n",
    "# --- Outlier detection and removal (IQR method) ---\n",
    "def remove_outliers_iqr(df, value_col, group_col):\n",
    "    df_clean = pd.DataFrame()\n",
    "    for label in df[group_col].unique():\n",
    "        group = df[df[group_col] == label]\n",
    "        if len(group) > 0:\n",
    "            Q1 = group[value_col].quantile(0.25)\n",
    "            Q3 = group[value_col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            outliers = group[(group[value_col] < lower) | (group[value_col] > upper)]\n",
    "            if not outliers.empty:\n",
    "                print(f\"{label}: {len(outliers)} outlier(s) removed. Values: {outliers[value_col].values}\")\n",
    "            group_no_outliers = group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "            df_clean = pd.concat([df_clean, group_no_outliers], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{label}: No data.\")\n",
    "    return df_clean\n",
    "\n",
    "# Remove outliers from df_total_green\n",
    "df_no_outliers = remove_outliers_iqr(\n",
    "    df, value_col='Sum Green Area', group_col='Well_Group_Label'\n",
    ")\n",
    "# Plot histogram and KDE for each group\n",
    "plt.figure(figsize=(12, 6))\n",
    "for label in order:\n",
    "    group_data = combined_results[combined_results['Well_Group_Label'] == label]['Sum Green Area (corrected)']\n",
    "    if not group_data.empty:\n",
    "        sns.histplot(group_data, kde=True, label=label, alpha=0.5, bins=10)\n",
    "plt.legend(title=\"Treatment\")\n",
    "plt.title(\"Distribution of 'Sum Green Area (corrected)' by Treatment\")\n",
    "plt.xlabel(\"Sum Green Area (corrected) [µm²]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=df,\n",
    "    x='Well_Group_Label',\n",
    "    y='Sum Green Area',\n",
    "    ci='sd',\n",
    "    palette='viridis',\n",
    "    order=order  # Remove this line if you want automatic order\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x='Well_Group_Label',\n",
    "    y='Sum Green Area',\n",
    "    color='black',\n",
    "    size=6,\n",
    "    order=order,\n",
    "    jitter=True,\n",
    "    dodge=True,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('BxPC3 Green Area by Treatment')\n",
    "plt.xlabel('Treatment')\n",
    "plt.ylabel('Sum Green Area (corrected) [µm²]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from scipy.stats import shapiro, kruskal\n",
    "\n",
    "# Shapiro-Wilk test for normality per group\n",
    "# -- Classic normality test, but only valid for n > 2. Useful for deciding between parametric/non-parametric stats.\n",
    "for label in order:\n",
    "    group_data = df[df['Well_Group_Label'] == label]['Sum Green Area (corrected)']\n",
    "    if len(group_data) > 2:\n",
    "        stat, p = shapiro(group_data)\n",
    "        print(f\"{label}: Shapiro-Wilk p-value = {p:.4f} (n={len(group_data)})\")\n",
    "    else:\n",
    "        print(f\"{label}: Not enough data for Shapiro-Wilk test (n={len(group_data)})\")\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric ANOVA)\n",
    "group_data = [df[df['Well_Group_Label'] == label]['Sum Green Area (corrected)'].values for label in order]\n",
    "group_data_filtered = [g for g in group_data if len(g) > 2]\n",
    "labels_filtered = [label for g, label in zip(group_data, order) if len(g) > 2]\n",
    "\n",
    "if len(group_data_filtered) > 1:\n",
    "    stat, p = kruskal(*group_data_filtered)\n",
    "    print(f\"\\nKruskal-Wallis H-statistic: {stat:.4f}\")\n",
    "    print(f\"p-value: {p:.4e}\")\n",
    "    if p < 0.05:\n",
    "        print(\"Statistically significant difference between at least two groups.\")\n",
    "    else:\n",
    "        print(\"No statistically significant difference detected between groups.\")\n",
    "else:\n",
    "    print(\"Not enough groups with sufficient data for Kruskal-Wallis test.\")\n",
    "\n",
    "# Only keep groups present in your data and with at least 3 values\n",
    "valid_labels = [label for label in order if len(df[df['Well_Group_Label'] == label]) > 2]\n",
    "filtered = df[df['Well_Group_Label'].isin(valid_labels)]\n",
    "\n",
    "# Dunn's test (pairwise, Bonferroni correction)\n",
    "dunn_results = sp.posthoc_dunn(\n",
    "    filtered,\n",
    "    val_col='Sum Green Area (corrected)',\n",
    "    group_col='Well_Group_Label',\n",
    "    p_adjust='bonferroni'\n",
    ")\n",
    "\n",
    "# Print p-values for each group vs UT\n",
    "print(\"\\nPairwise Dunn's test p-values vs UT:\")\n",
    "for label in valid_labels:\n",
    "    if label != \"UT\":\n",
    "        try:\n",
    "            pval = dunn_results.loc[label, \"UT\"]\n",
    "        except KeyError:\n",
    "            pval = dunn_results.loc[\"UT\", label]\n",
    "        print(f\"{label} vs UT: p = {pval:.4e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
